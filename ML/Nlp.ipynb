{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56df6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5560ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/xantanium/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/xantanium/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/xantanium/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/xantanium/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6052930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Tokenization is the first step of text processing. It involves breaking down text into words or sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6c1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'of', 'text', 'processing', '.', 'It', 'involves', 'breaking', 'down', 'text', 'into', 'words', 'or', 'sentences']\n",
      "Sentence Tokens: ['Tokenization is the first step of text processing.', 'It involves breaking down text into words or sentences']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Tokenize\n",
    "2. Remove stopwords and puncts\n",
    "3. Stem and lemmatize\n",
    "4. pos tag\n",
    "5. tfidf\n",
    "'''\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(doc)\n",
    "sentences = sent_tokenize(doc)\n",
    "\n",
    "print(f\"Word Tokens: {words}\")\n",
    "print(f\"Sentence Tokens: {sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdb481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Tokenization', 'first', 'step', 'text', 'processing', 'It', 'involves', 'breaking', 'text', 'words', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "stops = set(stopwords.words('english'))\n",
    "puncs = set(string.punctuation)\n",
    "\n",
    "filtered = [word for word in words if word not in stops and word not in puncs]\n",
    "print(f\"Filtered Words: {filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0874f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['token', 'first', 'step', 'text', 'process', 'it', 'involv', 'break', 'text', 'word', 'sentenc']\n",
      "Lemmatized Words: ['Tokenization', 'first', 'step', 'text', 'processing', 'It', 'involves', 'breaking', 'text', 'word', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in filtered]\n",
    "print(f\"Stems: {stems}\")\n",
    "\n",
    "lemms = [lemma.lemmatize(word) for word in filtered]\n",
    "print(f\"Lemmatized Words: {lemms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f48ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('Tokenization', 'NN'), ('first', 'RB'), ('step', 'VB'), ('text', 'RB'), ('processing', 'VBG'), ('It', 'PRP'), ('involves', 'VBZ'), ('breaking', 'VBG'), ('text', 'NN'), ('words', 'NNS'), ('sentences', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# pos-tagging\n",
    "pos_tags = pos_tag(filtered)\n",
    "print(f\"POS tags: {pos_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0565b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: [[0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.45883147 0.22941573 0.22941573 0.22941573]]\n",
      "Feature names: ['breaking' 'down' 'first' 'into' 'involves' 'is' 'it' 'of' 'or'\n",
      " 'processing' 'sentences' 'step' 'text' 'the' 'tokenization' 'words']\n"
     ]
    }
   ],
   "source": [
    "# TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([doc])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(f\"TFIDF: {tfidf_array}\")\n",
    "print(f\"Feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ba9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
